{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules\n",
    "from src.main_utils import Configuration\n",
    "from src.evaluation_utils import eval_epoch\n",
    "from src.thirdHand_data_loader import get_min_max_from_dataset\n",
    "\n",
    "# custom visualizations\n",
    "from src.motion_visualization_tools import *\n",
    "from src.cvae_networks import *\n",
    "from src.train_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = Configuration()\n",
    "print(model_config.data_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 or 5 both work!\n",
    "kernel_size = 5\n",
    "\n",
    "# between 10 and 5\n",
    "first_filter_size = 9\n",
    "\n",
    "# depth should be 2, 3, 4\n",
    "depth = 2\n",
    "dropout = 0.1\n",
    "epochs = 10 #300\n",
    "\n",
    "batch_size = 128\n",
    "latent_dim = 8\n",
    "\n",
    "rec_loss= \"L1\"\n",
    "reduction= \"sum\" \n",
    "kld_weight = 1e-1\n",
    "\n",
    "model_name_to_save=\"c_vae_model_tmp\"\n",
    "\n",
    "model = VAE_CNN(model_config.device, \n",
    "                first_filter_size, \n",
    "                kernel_size, \n",
    "                depth, \n",
    "                dropout,\n",
    "                latent_dim,\n",
    "                rec_loss= rec_loss,\n",
    "                reduction= reduction,\n",
    "                kld_weight = kld_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses ,train_rec_losses,train_kld_losses, eval_losses = train_model (model, model_config, epochs, model_name_to_save) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scaling_method(model_config)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generation_method(model, model_config)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data= None\n",
    "sample_size = np.random.randint(0, 128, size= 16)\n",
    "\n",
    "for d in model_config.train_iterator:\n",
    "    x_samples = d[model_config.data_item][sample_size, :,:]\n",
    "    y_sampels = d['Y'][sample_size, :,:] \n",
    "    \n",
    "    z, __, __ = model.encoder(x_samples, y_sampels)\n",
    "\n",
    "    noise = torch.normal(mean=.1, std=.21, size = z.shape).to(model_config.device)\n",
    "    z = z + noise\n",
    " \n",
    "    x_generated = model.decoder(z , y_sampels)\n",
    "    x_rec,__, __ = model(x_samples, y_sampels)\n",
    "    \n",
    "    fig_1 = show_generated_motions(x_samples, \"Original\")\n",
    "    fig_1.show()\n",
    "    fig_2 = show_generated_motions(x_rec, \"Reconstration\")\n",
    "    fig_2.show()\n",
    "    fig_3 = show_generated_motions(x_generated, \"Generated\")\n",
    "    fig_3.show()\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for d in model_config.train_iterator:\n",
    "    if counter > 6:\n",
    "        X = d[\"X\"][:3, :, :]\n",
    "        print(X.shape)\n",
    "        min_val, max_val = get_min_max_from_dataset(X)\n",
    "            \n",
    "        X_scaled = torch.zeros_like(X)\n",
    "        X_centered = torch.zeros_like(X)\n",
    "        X_centered_scaled = torch.zeros_like(X)\n",
    "\n",
    "        # # finding the touching point of each motion and centering the motion on that\n",
    "        center_points = torch.zeros_like(X[:, 0:1, :])        \n",
    "        center_points[:, 0, :3] = X[:, 9, :3]\n",
    "        X_centered =  X - center_points\n",
    "\n",
    "        # # scaling data between 0 and 1\n",
    "        X_scaled =(X - min_val) / (max_val - min_val)\n",
    "        \n",
    "        print (torch.sum(X -(X_scaled*(max_val - min_val)+min_val)) < 0.001)\n",
    "        \n",
    "        # # scaling the centered data between 0 and 1\n",
    "        centered_min_val, centered_max_val = get_min_max_from_dataset(X_centered)\n",
    "        \n",
    "        x_samples = d[model_config.data_item]\n",
    "        y_samples = d[\"Y\"]\n",
    "        \n",
    "        x_rec, __, __ = model(x_samples, y_samples)\n",
    "        \n",
    "        centered_min_val= d[\"centered_min_val\"][0]\n",
    "        centered_max_val= d[\"centered_max_val\"][0]\n",
    "        \n",
    "        X_centered_scaled = (X_centered - centered_min_val) / (centered_max_val - centered_min_val)\n",
    "        X_back_to_orig = (X_centered_scaled*(centered_max_val - centered_min_val)+centered_min_val)+center_points\n",
    "            \n",
    "\n",
    "        fig_01 = compare_motion_data_plots([X, X_back_to_orig], 0) \n",
    "        break\n",
    "    counter +=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('ThirdHand')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32d9db1937a742deca98e81c7250077b3dbbb3d3b598369d9a0a4746ee2520b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
